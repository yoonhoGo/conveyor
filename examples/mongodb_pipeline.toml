# MongoDB ETL Pipeline Example
# This pipeline extracts data from MongoDB, transforms it, and loads into another format
# Demonstrates cursor-based pagination for large datasets

[pipeline]
name = "mongodb_etl"
version = "1.0.0"
description = "Extract from MongoDB with cursor pagination and transform data"

[global]
log_level = "info"
max_parallel_tasks = 2
timeout_seconds = 600
plugins = ["mongodb"]  # Load MongoDB plugin dynamically

# MongoDB Source - First Page with Cursor Pagination
[[sources]]
name = "user_events"
type = "mongodb"

[sources.config]
connection_string = "mongodb://localhost:27017"
database = "analytics"
collection = "raw_events"
query = '{ "event_type": "purchase" }'
# Cursor-based pagination settings
cursor_field = "_id"          # Use _id for pagination
# cursor_value = ""           # Omit for first page, or set to last _id from previous batch
batch_size = 5000             # Fetch 5000 documents per batch
limit = 10000                 # Optional: total limit across all batches

# Transformations
[[transforms]]
name = "validate_data"
function = "validate_schema"

[transforms.config]
required_fields = ["_id", "event_type", "user_id", "amount"]
non_nullable = ["_id", "user_id"]

[transforms.config.field_types]
user_id = "string"
amount = "int"

[[transforms]]
name = "filter_high_value"
function = "filter"

[transforms.config]
column = "amount"
operator = ">="
value = 100.0

[[transforms]]
name = "add_tax"
function = "map"

[transforms.config]
expression = "amount * 1.1"
output_column = "amount_with_tax"

# Output to JSON file
[[sinks]]
name = "output_json"
type = "json"

[sinks.config]
path = "output/processed_events.json"
format = "jsonl"
pretty = false

# Also output to stdout for preview
[[sinks]]
name = "preview"
type = "stdout"

[sinks.config]
format = "table"
limit = 10

# Error Handling
[error_handling]
strategy = "retry"
max_retries = 5
retry_delay_seconds = 10

[error_handling.dead_letter_queue]
enabled = true
path = "errors/mongodb_errors/"