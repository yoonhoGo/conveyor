# MongoDB ETL Pipeline Example (DAG Format)
# This pipeline extracts data from MongoDB, transforms it, and loads into another format
# Demonstrates cursor-based pagination for large datasets

[pipeline]
name = "mongodb_etl"
version = "1.0.0"
description = "Extract from MongoDB with cursor pagination and transform data"

[global]
log_level = "info"
max_parallel_tasks = 2
timeout_seconds = 600
plugins = ["mongodb"]  # Load MongoDB plugin dynamically

# Stage 1: MongoDB Source - Fetch purchase events
[[stages]]
id = "user_events"
function = "mongodb-find"  # MongoDB plugin function
inputs = []

[stages.config]
uri = "mongodb://localhost:27017"
database = "analytics"
collection = "raw_events"
query = '{ "event_type": "purchase" }'
limit = 10000

# Stage 2: Validate schema
[[stages]]
id = "validate_data"
function = "validate.schema"
inputs = ["user_events"]

[stages.config]
required_fields = ["_id", "event_type", "user_id", "amount"]
non_nullable = ["_id", "user_id"]

[stages.config.field_types]
user_id = "string"
amount = "int"

# Stage 3: Filter high value events
[[stages]]
id = "filter_high_value"
function = "filter.apply"
inputs = ["validate_data"]

[stages.config]
column = "amount"
operator = ">="
value = 100.0

# Stage 4: Add tax calculation
[[stages]]
id = "add_tax"
function = "map.apply"
inputs = ["filter_high_value"]

[stages.config]
expression = "amount * 1.1"
output_column = "amount_with_tax"

# Stage 5: Output to JSON file
[[stages]]
id = "output_json"
function = "json.write"
inputs = ["add_tax"]

[stages.config]
path = "output/processed_events.json"
format = "jsonl"
pretty = false

# Stage 6: Also output to stdout for preview (branching)
[[stages]]
id = "preview"
function = "stdout.write"
inputs = ["add_tax"]

[stages.config]
format = "table"
limit = 10

# Error Handling
[error_handling]
strategy = "retry"
max_retries = 5
retry_delay_seconds = 10

[error_handling.dead_letter_queue]
enabled = true
path = "errors/mongodb_errors/"
