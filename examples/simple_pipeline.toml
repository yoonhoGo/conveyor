# Simple ETL Pipeline Example (DAG Format)
# This pipeline reads CSV data, performs basic transformations, and outputs to JSON

[pipeline]
name = "simple_data_processing"
version = "1.0.0"
description = "Basic CSV to JSON transformation pipeline with filtering"

[global]
log_level = "info"
max_parallel_tasks = 4
timeout_seconds = 300

# Stage 1: Load CSV data
[[stages]]
id = "sales_data"
type = "source.csv"
inputs = []

[stages.config]
path = "data/sales.csv"
headers = true
delimiter = ","

# Stage 2: Validate schema
[[stages]]
id = "validate_data"
type = "transform.validate_schema"
inputs = ["sales_data"]

[stages.config]
required_fields = ["id", "date", "amount", "customer_id"]
date_fields = ["date"]

# Stage 3: Filter high value sales
[[stages]]
id = "filter_high_value"
type = "transform.filter"
inputs = ["validate_data"]

[stages.config]
column = "amount"
operator = ">="
value = 1000.0

# Stage 4: Add tax calculation
[[stages]]
id = "add_tax"
type = "transform.map"
inputs = ["filter_high_value"]

[stages.config]
expression = "amount * 1.1"
output_column = "amount_with_tax"

# Stage 5: Save to JSON
[[stages]]
id = "output_json"
type = "sink.json"
inputs = ["add_tax"]

[stages.config]
path = "output/processed_sales.json"
format = "records"
pretty = true

# Stage 6: Also save to CSV (branching from add_tax)
[[stages]]
id = "high_value_csv"
type = "sink.csv"
inputs = ["add_tax"]

[stages.config]
path = "output/high_value_sales.csv"
headers = true

# Error Handling
[error_handling]
strategy = "continue"  # Options: "stop", "continue", "retry"
max_retries = 3
retry_delay_seconds = 5

[error_handling.dead_letter_queue]
enabled = true
path = "errors/"