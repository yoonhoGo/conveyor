# Conveyor

A high-performance, TOML-based ETL (Extract, Transform, Load) CLI tool built in Rust for data pipeline processing.

## Features

- **ğŸ“‹ TOML Configuration**: Simple, declarative pipeline definitions
- **âš¡ High Performance**: Built with Rust and Polars for fast data processing
- **ğŸ”Œ Extensible**: Modular architecture with pluggable data sources, transforms, and sinks
- **ğŸ”„ Async Processing**: Built on Tokio for efficient concurrent operations
- **ğŸ›¡ï¸ Type-Safe**: Rust's type system ensures reliability and safety
- **ğŸ“Š Multiple Data Formats**: Support for CSV, JSON, and more

## Installation

### Prerequisites

- Rust 1.70 or higher
- Cargo

### Build from Source

```bash
git clone https://github.com/yourusername/conveyor.git
cd conveyor
cargo build --release
```

The binary will be available at `target/release/conveyor`.

## Quick Start

### 1. Create a Pipeline Configuration

Create a file named `pipeline.toml`:

```toml
[pipeline]
name = "my_first_pipeline"
version = "1.0.0"
description = "A simple data transformation pipeline"

[global]
log_level = "info"
max_parallel_tasks = 4
timeout_seconds = 300

[[sources]]
name = "input_data"
type = "csv"

[sources.config]
path = "data/input.csv"
headers = true
delimiter = ","

[[transforms]]
name = "filter_data"
function = "filter"

[transforms.config]
column = "amount"
operator = ">="
value = 100.0

[[transforms]]
name = "add_tax"
function = "map"

[transforms.config]
expression = "amount * 1.1"
output_column = "amount_with_tax"

[[sinks]]
name = "output_json"
type = "json"

[sinks.config]
path = "output/result.json"
format = "records"
pretty = true

[error_handling]
strategy = "stop"
max_retries = 3
retry_delay_seconds = 5
```

### 2. Run the Pipeline

```bash
conveyor run -c pipeline.toml
```

## Usage

### Commands

#### Run a Pipeline

```bash
conveyor run --config <path-to-config.toml>
```

Options:
- `--dry-run`: Validate configuration without executing
- `--continue-on-error`: Continue pipeline execution even if errors occur

#### Validate Configuration

```bash
conveyor validate --config <path-to-config.toml>
```

#### List Available Modules

```bash
conveyor list
conveyor list --module-type sources
```

#### Generate Sample Configuration

```bash
conveyor generate --output sample-pipeline.toml
```

## Modules

### Data Sources

| Module | Description | Configuration |
|--------|-------------|---------------|
| `csv` | Read CSV files | `path`, `headers`, `delimiter` |
| `json` | Read JSON files | `path`, `format` (records/jsonl/dataframe) |
| `stdin` | Read from standard input | `format` (json/jsonl/csv/raw) |

### Transforms

| Function | Description | Configuration |
|----------|-------------|---------------|
| `filter` | Filter rows based on conditions | `column`, `operator`, `value` |
| `map` | Create or transform columns | `expression`, `output_column` |
| `validate_schema` | Validate data schema | `required_fields`, `field_types`, `non_nullable` |

### Sinks

| Module | Description | Configuration |
|--------|-------------|---------------|
| `csv` | Write to CSV files | `path`, `headers`, `delimiter` |
| `json` | Write to JSON files | `path`, `format`, `pretty` |
| `stdout` | Write to standard output | `format` (table/json/jsonl/csv), `limit` |

## Configuration Reference

### Pipeline Section

```toml
[pipeline]
name = "pipeline_name"          # Required
version = "1.0.0"               # Required
description = "Description"     # Optional
```

### Global Section

```toml
[global]
log_level = "info"              # trace, debug, info, warn, error
max_parallel_tasks = 4          # Number of concurrent tasks
timeout_seconds = 300           # Pipeline timeout
```

### Sources

```toml
[[sources]]
name = "unique_name"            # Unique identifier
type = "csv"                    # Module type

[sources.config]
# Module-specific configuration
```

### Transforms

```toml
[[transforms]]
name = "unique_name"            # Unique identifier
function = "filter"             # Transform function
input = "source_name"           # Optional: specify input

[transforms.config]
# Transform-specific configuration
```

### Sinks

```toml
[[sinks]]
name = "unique_name"            # Unique identifier
type = "json"                   # Module type
input = "transform_name"        # Optional: specify input

[sinks.config]
# Module-specific configuration
```

### Error Handling

```toml
[error_handling]
strategy = "stop"               # stop, continue, retry
max_retries = 3                 # Number of retry attempts
retry_delay_seconds = 5         # Delay between retries

[error_handling.dead_letter_queue]
enabled = true
path = "errors/"
```

## Examples

See the `examples/` directory for complete pipeline configurations:

- `simple_pipeline.toml` - Basic CSV to JSON transformation
- `mongodb_pipeline.toml` - MongoDB ETL operations

## Development

### Running Tests

```bash
# Run all tests
cargo test

# Run only unit tests
cargo test --lib

# Run with output
cargo test -- --nocapture
```

### Building

```bash
# Debug build
cargo build

# Release build (optimized)
cargo build --release

# With specific features
cargo build --features "mongodb,postgres"
```

## Architecture

```
conveyor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli/              # Command-line interface
â”‚   â”œâ”€â”€ core/             # Core pipeline engine
â”‚   â”‚   â”œâ”€â”€ config.rs     # Configuration parsing
â”‚   â”‚   â”œâ”€â”€ error.rs      # Error types
â”‚   â”‚   â”œâ”€â”€ pipeline.rs   # Pipeline executor
â”‚   â”‚   â”œâ”€â”€ registry.rs   # Module registry
â”‚   â”‚   â””â”€â”€ traits.rs     # Core traits
â”‚   â”œâ”€â”€ modules/          # ETL modules
â”‚   â”‚   â”œâ”€â”€ sources/      # Data sources
â”‚   â”‚   â”œâ”€â”€ transforms/   # Data transformations
â”‚   â”‚   â””â”€â”€ sinks/        # Data sinks
â”‚   â””â”€â”€ utils/            # Utilities
â”œâ”€â”€ examples/             # Example pipelines
â”œâ”€â”€ tests/                # Integration tests
â””â”€â”€ data/                 # Test data
```

## Performance

Conveyor is built with performance in mind:

- **Rust**: Zero-cost abstractions and memory safety
- **Polars**: High-performance DataFrame library (10-100x faster than Pandas)
- **Tokio**: Efficient async runtime for I/O operations
- **Arrow**: Columnar memory format for optimal data processing

Benchmark comparisons with Python-based ETL tools show 10-100x performance improvements for typical workloads.

## Roadmap

- [x] Core pipeline engine
- [x] CSV, JSON data sources
- [x] Basic transforms (filter, map, validate)
- [x] File-based sinks
- [ ] Database connectors (MongoDB, PostgreSQL, MySQL)
- [ ] HTTP data sources
- [ ] Advanced transforms (aggregate, join, pivot)
- [ ] Stream processing support
- [ ] Plugin system for custom modules
- [ ] Web UI for pipeline monitoring
- [ ] Distributed execution

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Built with [Rust](https://www.rust-lang.org/)
- Powered by [Polars](https://www.pola.rs/) for data processing
- Uses [Tokio](https://tokio.rs/) for async runtime
- CLI built with [Clap](https://docs.rs/clap/)

## Support

- ğŸ“– [Documentation](https://docs.example.com/conveyor)
- ğŸ› [Issue Tracker](https://github.com/yourusername/conveyor/issues)
- ğŸ’¬ [Discussions](https://github.com/yourusername/conveyor/discussions)

---

**Made with â¤ï¸ and Rust**
